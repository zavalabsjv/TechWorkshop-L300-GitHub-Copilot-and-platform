---
title: '4. Deploy an Observability Workbook'
layout: default
nav_order: 3
parent: 'Exercise 06: AI Governance and Model Observability'
---

# Task 04 - Deploy an Observability Workbook

## Introduction

Azure Workbooks help you visualize operational diagnostics for your Azure AI Services resource. 

## Description

In this task, you will create and deploy an Azure Workbook that visualizes operational diagnostics for your Azure AI Services resource. The workbook uses only platform diagnostics emitted to AzureDiagnostics, without any application-level telemetry.

You will first validate each KQL query manually in Log Analytics, then use GitHub Copilot to generate the Bicep scaffolding and placeholder JSON, and finally paste in the provided workbook definition and deploy it to your environment.

## Success Criteria

You have successfully completed Task 04 when:

1. All three KQL queries return data in Log Analytics when run manually:

- Request volume

- Latency percentiles

- Operation-name breakdown

2. The workbook deploys successfully using your Bicep template and the provided JSON.
3. The workbook appears under: Azure Monitor → Workbooks → AI Services Observability
4. All three visuals load and display real data:
   - Request volume over time
   - Latency percentiles (p50/p95/p99)
   - Requests by operation name over time

## Key Tasks

### 01: Run Diagnostic Queries Manually

These queries confirm your Azure AI Services resource is emitting data to Log Analytics.

1. Request Volume: You should see a time series showing your model invocation volume.

```kusto
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where Category == "RequestResponse"
| summarize Requests = count() by bin(TimeGenerated, 5m)
| order by TimeGenerated asc
```

2. Latency Percentiles: You should see p50/p95/p99 latency patterns for all model calls.

```kusto
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where Category == "RequestResponse"
| summarize
    p50 = percentiles(DurationMs, 50),
    p95 = percentiles(DurationMs, 95),
    p99 = percentiles(DurationMs, 99)
  by bin(TimeGenerated, 5m)
| order by TimeGenerated asc
```

3. Breakdown by operation name: This shows the number of requests by operation name.

```kusto
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where Category == "RequestResponse"
| extend Operation = OperationName
| summarize Count = count() by bin(TimeGenerated, 5m), Operation
| order by TimeGenerated asc
```

<details markdown="block">
<summary>Run diagnostic queries step-by-step</summary>

1. Navigate to your Log Analytics workspace in the Azure portal.
2. Open the Logs query editor.
3. Run the Request Volume query. You should see a time series showing your model invocation volume.
4. Run the Latency Percentiles query. You should see p50/p95/p99 latency patterns for all model calls.
5. Run the Breakdown by operation name query. This shows the number of requests by operation name.

</details>

### 02: Use Copilot to Create Workbook Scaffolding

Open your project folder in VS Code and prompt GitHub Copilot:

```text
Generate a workbook.bicep template and a placeholder workbook JSON file for an Azure Workbook named "AI Services Observability."
The workbook will visualize three queries: request volume, latency percentiles, and breakdown by operation name.
Use a Log Analytics workspace as the data source.
```

Copilot will produce:

- workbook.bicep
- workbook.json (placeholder)

<details markdown="block">
<summary>Example Steps</summary>

1. Open your project folder in VS Code.
2. Prompt GitHub Copilot with the prompt above.
3. Review the generated workbook.bicep and workbook.json files.
4. If the generated files look correct, approve them.

Note: Do not provision the workbook yet, proceed to the next step.

</details>

### 03: Replace the Placeholder JSON

Overwrite the placeholder with the final workbook definition shown here:

```json
{
  "version": "Notebook/1.0",
  "items": [
    {
      "type": 1,
      "content": {
        "json": "## AI Services Observability\nThis workbook visualizes Azure AI Services operational data using platform diagnostics from the AzureDiagnostics table.\n\nIt includes:\n- Request volume over time\n- Latency percentiles (p50/p95/p99)\n- Breakdown by operation name"
      },
      "name": "text - 0"
    },
    {
      "type": 3,
      "content": {
        "version": "KqlItem/1.0",
        "query": "AzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.COGNITIVESERVICES\"\n| where Category == \"RequestResponse\"\n| summarize Requests = count() by bin(TimeGenerated, 5m)\n| order by TimeGenerated asc",
        "size": 1,
        "title": "Request Volume Over Time",
        "queryType": 0,
        "resourceType": "microsoft.operationalinsights/workspaces",
        "visualization": "timechart",
        "chartSettings": {
          "yAxis": [
            "Requests"
          ]
        }
      },
      "name": "RequestVolume"
    },
    {
      "type": 3,
      "content": {
        "version": "KqlItem/1.0",
        "query": "AzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.COGNITIVESERVICES\"\n| where Category == \"RequestResponse\"\n| summarize\n p50 = percentiles(DurationMs, 50),\n p95 = percentiles(DurationMs, 95),\n p99 = percentiles(DurationMs, 99)\n by bin(TimeGenerated, 5m)\n| order by TimeGenerated asc",
        "size": 1,
        "title": "Latency Percentiles (p50 / p95 / p99)",
        "queryType": 0,
        "resourceType": "microsoft.operationalinsights/workspaces",
        "visualization": "timechart"
      },
      "name": "LatencyTrends"
    },
    {
      "type": 3,
      "content": {
        "version": "KqlItem/1.0",
        "query": "AzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.COGNITIVESERVICES\"\n| where Category == \"RequestResponse\"\n| extend Operation = OperationName\n| summarize Count = count() by bin(TimeGenerated, 5m), Operation\n| order by TimeGenerated asc",
        "size": 1,
        "title": "Requests by Operation Name Over Time",
        "queryType": 0,
        "resourceType": "microsoft.operationalinsights/workspaces",
        "visualization": "timechart",
        "chartSettings": {
          "yAxis": [
            "Count"
          ]
        }
      },
      "name": "OperationBreakdown"
    }
  ],
  "fallbackResourceIds": [
    "azureResourceId"
  ],
  "$schema": "https://github.com/Microsoft/Application-Insights-Workbooks/blob/master/schema/workbook.json"
}
```

<details markdown="block">
<summary>Example Steps</summary>

1. Open your project folder in VS Code.
2. Open the workbook.json file.
3. Overwrite the file with the content shown above.
4. Save the file.

</details>

### 04: Deploy the Workbook

Deploy using `azd provision`

```bash
azd provision
```

<details markdown="block">
<summary>Example Steps</summary>

1. Run `azd provision` to deploy the workbook.
2. Once the deployment is complete, check the Azure portal to confirm the workbook is deployed.
</details>

### 05: Review in the Portal

Navigate to: Azure Monitor → Workbooks → AI Services Observability

Confirm that the workbook loads and all visuals render.

Confirm that the data matches the output from your manual KQL queries

<details markdown="block">
<summary>Example Steps</summary>

1. Open the Azure portal.
2. Navigate to Azure Monitor → Workbooks → AI Services Observability.
3. Confirm that the workbook loads and all visuals render.
4. Confirm that the data matches the output from your manual KQL queries.
5. Close the workbook.

</details>

## Summary
You've completed this task. You deployed an Azure Monitor workbook that visualizes operational diagnostics for your Azure AI Services resource. You used Copilot to create the workbook scaffolding and replace the placeholder JSON. You also validated the workbook by running manual KQL queries and reviewing the data in the portal.